# streamlit_app.py
import streamlit as st

# ------------------------------------------------------------------
# 1.  THE FULL 269-QUESTION DATASET (exact text you supplied)
# ------------------------------------------------------------------
RAW = """Data Governance\tWhich of the following is NOT part of the Data Governance lifecycle process?\tHardware procurement\tPolicy setting\tIssue management\tData asset valuation\tHardware procurement
Data Governance\tThese are two metrics you must produce to track the effectiveness of your Data Governance program\tImproved efficiency in operations\tReduction of risk\tMarketing campaign ROI\tNumber of servers deployed\tReduction of risk
Data Governance\tManaging Data Stewardship\tIs solely IT’s job\tDocuments business rules\tManages data quality issues\tCreates core metadata\tDocuments business rules
Data Governance\tThe Data Governance Operating Framework\tShould ignore culture\tMust define accountabilities\tIs a one-time project\tOnly applies to IT systems\tMust define accountabilities
Data Governance\tShould Data Policies govern Business Glossary artefacts?\tYes, policies guide glossary content\tNo, glossaries are optional\tOnly for regulators\tPolicies should never mention glossaries\tYes, policies guide glossary content
Data Governance\tWhich of the following is one of the ethical principles for Data Governance per DMBOK?\tShared responsibility\tMaximize profit at any cost\tData ownership by IT only\tAvoid transparency\tShared responsibility
Data Governance\tWhat does a Business Glossary repository store?\tHardware inventory\tAgreed business term definitions\tSoftware licenses\tEmployee salaries\tAgreed business term definitions
Data Governance\tWhich of these is NOT a Data Governance discipline?\tPolicy setting\tHardware procurement\tIssue management\tData asset valuation\tHardware procurement
Data Governance\tWhich best describes the relationship between Data Governance and IT Governance?\tDG is a subset of IT Governance\tDG focuses on data assets, ITG on IT portfolio\tThey are identical\tDG only applies to finance\tDG focuses on data assets, ITG on IT portfolio
Data Governance\tWhen performing Data Quality analysis, which error is commonly surfaced?\tDuplicate hardware purchases\tInconsistent customer identifiers\tExcess office space\tMarketing campaign delays\tInconsistent customer identifiers
Data Governance\tWhich is the purpose of Data Stewardship?\tManage data on behalf of stakeholders\tProcure cloud storage\tSet hardware budgets\tRun marketing campaigns\tManage data on behalf of stakeholders
Data Governance\tWhich metadata example does NOT represent a Business Glossary entry?\tCustomer = active client\tServer model = Dell R740\tRevenue = recognized income\tOrder date = purchase date\tServer model = Dell R740
Data Governance\tWhich data-quality issue is NOT prevented by enforcing Data Standards?\tInvalid date formats\tAmbiguous business terms\tHardware failures\tDuplicate customer records\tHardware failures
Data Governance\tWhich is NOT a common functional requirement for a Business Glossary solution?\tSearchable term definitions\tRole-based access control\tHardware temperature monitoring\tVersion control of definitions\tHardware temperature monitoring
Data Governance\tWhat is NOT true about Data Asset Valuation?\tIt considers replacement cost\tIt includes market value estimates\tIt ignores risk costs\tIt factors identified opportunities\tIt ignores risk costs
Data Governance\tData Governance Council\tApproves hardware purchases\tOversees policy adoption\tRuns daily IT operations\tDesigns office layouts\tOversees policy adoption
Data Governance\tWhat are key reasons Data Governance needs C-level sponsorship?\tDrive cultural change\tProcure laptops\tSet marketing slogans\tManage office catering\tDrive cultural change
Data Governance\tList of terms: Chief Data Steward, Business Data Steward, Technical Data Steward\tAll are Data Steward types\tAll are hardware roles\tOnly first is a real role\tOnly last is a real role\tAll are Data Steward types
Data Governance\tWhat is NOT a goal of a Data Governance program?\tMaximize data value\tEnsure regulatory compliance\tIncrease hardware resale value\tReduce data risks\tIncrease hardware resale value
Data Governance\tHow should Data Policies be communicated?\tVia posters in break rooms only\tEffectively, monitored, and enforced\tStored in a locked drawer\tVia yearly email blast\tEffectively, monitored, and enforced
Data Governance\tWhich is NOT an activity for Data Stewardship?\tCreating metadata definitions\tProcuring servers\tDocumenting business rules\tResolving data quality issues\tProcuring servers
Data Governance\tService Level Agreement for Data Quality\tDefines hardware refresh cycles\tSets data quality targets\tOutlines office cleaning schedules\tSpecifies catering menus\tSets data quality targets
Data Governance\tHow does Data Governance differ from IT Governance?\tDG manages data assets, ITG manages IT portfolio\tDG is subset of ITG\tNo difference\tDG only governs finance\tDG manages data assets, ITG manages IT portfolio
Data Governance\tWhen selecting a Business Glossary tool\tIgnore integration needs\tPrioritize ease of search and workflow\tChoose the cheapest hardware\tFocus on color themes\tPrioritize ease of search and workflow
Data Governance\tWhich statement is NOT true of designing a Data Governance framework?\tIt must be tailored to culture\tIt should ignore regulatory impact\tIt needs executive sponsorship\tIt defines accountabilities\tIt should ignore regulatory impact
Data Governance\tWhich is TRUE about defining high-quality Business Glossary terms?\tDefinitions should be ambiguous\tDefinitions must be clear and rigorous\tHardware specs should be included\tTerms can contradict each other\tDefinitions must be clear and rigorous
Data Governance\tWhat is a major difference between a Centralized and Federated Data Governance model?\tCentralized uses one body, federated coordinates with multiple units\tFederated ignores standards\tCentralized only governs hardware\tNo difference\tCentralized uses one body, federated coordinates with multiple units
Data Governance\tWhat is the role of the Chief Data Steward?\tManage office supplies\tOversee data governance bodies\tDesign office layouts\tRun payroll\tOversee data governance bodies
Data Governance\tWhat is NOT a goal of Data Asset Valuation?\tEstimate replacement cost\tAssess market value\tTrack hardware depreciation\tIdentify income opportunities\tTrack hardware depreciation
Data Governance\tWhat are Reference Data Management tables?\tHardware inventory lists\tStandardized code value tables\tEmployee birthday calendars\tMarketing campaign charts\tStandardized code value tables
Data Governance\tKey difference between Policy and Procedure in Data Governance?\tPolicy tells 'what', procedure tells 'how'\tThey are synonyms\tPolicies apply only to hardware\tProcedures are optional\tPolicy tells 'what', procedure tells 'how'
Data Governance\tA Data Owner in a retail domain\tApproves data decisions for retail data\tOrders office furniture\tManages server racks\tDesigns marketing flyers\tApproves data decisions for retail data
Data Governance\tMain approach to managing Personal Identifiable Information (PII)\tIgnore privacy laws\tControl via policy and compliance monitoring\tStore on open file shares\tDelete all customer data\tControl via policy and compliance monitoring
Data Governance\tAn area around the edge of an organisation handling external data is known as?\tPerimeter data zone\tDMZ\tOffice lobby\tBreak room\tPerimeter data zone
Data Governance\tWhich principle is NOT part of sustainable Data Governance?\tOngoing commitment\tRigid one-time project mindset\tEmbedded in processes\tMeasured impact\tRigid one-time project mindset
Data Governance\t3 Vs of big data; which V relates to data accuracy?\tVolume\tVelocity\tVariety\tVeracity\tVeracity
Data Governance\tWhen creating an ethical data-handling strategy, which is NOT a concern?\tTransparency\tProfit maximization at any cost\tPrivacy compliance\tAccountability\tProfit maximization at any cost
Data Governance\tAccording to DMBOK, why is Data Governance critical?\tTo manage data as an asset\tTo buy more servers\tTo increase office space\tTo run marketing campaigns\tTo manage data as an asset
Data Governance\tCore idea of Data Governance per Seiner and Ladley?\tFocus on how decisions about data are made\tFocus on hardware refresh cycles\tIgnore data policies\tDelegate to marketing\tFocus on how decisions about data are made
Data Governance\tData profiling assists improving quality except?\tIdentifying hardware faults\tDiscovering null values\tSpotting format inconsistencies\tRevealing duplicate records\tIdentifying hardware faults
Data Governance\tHow do Executive Data Stewards participate?\tServe on Data Governance Council\tOrder office coffee\tManage server cooling\tDesign HR policies\tServe on Data Governance Council
Data Governance\tHighest level of data-model types is?\tConceptual data model\tServer rack diagram\tOffice floor plan\tMarketing funnel\tConceptual data model
Data Governance\tWhich is NOT a principle of organizations that get more value from data?\tData managed as corporate asset\tBest practices incented\tStrategy aligned with business\tIgnore data quality\tIgnore data quality
Data Governance\tA relationship label stating 'Each customer may place zero or more orders' expresses what cardinality rule?\tOne-to-one\tMany-to-many\tOne-to-many\tZero-to-zero\tOne-to-many
Data Governance\tDesign, implementation and support of stored data to maximize value across its lifecycle is known as?\tData Governance\tOffice management\tHardware provisioning\tMarketing strategy\tData Governance
Data Governance\tIn data modelling, a foreign key is?\tHardware serial number\tA field referencing another table's primary key\tOffice desk label\tMarketing campaign ID\tA field referencing another table's primary key
Data Modelling\tWhich of the following is NOT part of the system development lifecycle (SDLC)\tRequirements gathering\tTesting\tData modeling\tDeployment\tData modeling
Data Modelling\tThese are two metrics you must produce to track the effectiveness of your Data Model Scorecard\tModel Score and Percentage\tModel Score and Total Score\tComments and Percentage\tModel Score and Comments\tModel Score and Percentage
Data Modelling\tManaging Data Quality\tInvolves only data profiling\tRequires balancing short-term and long-term business interests\tIs solely the responsibility of the DBA\tFocuses only on data security\tRequires balancing short-term and long-term business interests
Data Modelling\tThe Data Model Scorecard\tIs only used for physical data models\tProvides 11 quality metrics\tMeasures only naming standards\tIs a type of data modeling tool\tProvides 11 quality metrics
Data Modelling\tShould a surrogate key be visible to end users of a database\tYes, always\tOnly in dimensional models\tOnly if it's a natural key\tNo, never\tNo, never
Data Modelling\tWhich of the following is one of the ethical principles for data governance according to the DMBOK\tData minimization\tData duplication\tIgnoring data quality\tMaximizing data collection\tData minimization
Data Modelling\tWhat does a Metadata Repository store\tOnly data values\tOnly physical data models\tDescriptive information about data models\tOnly business rules\tDescriptive information about data models
Data Modelling\tWhich of these is NOT a data modeling scheme\tRelational\tDimensional\tObject-Oriented\tSpreadsheet\tSpreadsheet
Data Modelling\tWhich of these best describes the relationship between conceptual and physical data models\tThey are identical\tConceptual models are more detailed\tPhysical models are technology-independent\tPhysical models adapt logical models to specific technology\tPhysical models adapt logical models to specific technology
Data Modelling\tWhen denormalizing data\tYou should never consider performance\tYou always introduce data errors\tYou should implement data quality checks\tIt has no impact on redundancy\tYou should implement data quality checks
Data Modelling\tWhich of these is the purpose of normalization\tTo increase redundancy\tTo improve query performance\tTo organize business complexity into stable data structures\tTo denormalize data\tTo organize business complexity into stable data structures
Data Modelling\tWhich of these metadata examples do NOT represent business metadata\tEntity definitions\tAttribute definitions\tPhysical data type of a column\tBusiness rules\tPhysical data type of a column
Data Modelling\tWhich of these data-quality issues is NOT prevented by normalization\tRedundancy\tUpdate anomalies\tInsert anomalies\tData type mismatches\tData type mismatches
Data Modelling\tWhich of these is NOT a common functional requirement for a data warehouse\tFast transaction processing\tComplex analytical queries\tHistorical data storage\tData integration\tFast transaction processing
Data Modelling\tWhat is NOT true about foreign keys\tThey represent relationships between entities\tThey are always part of the primary key\tThey appear in the child entity\tThey may be created implicitly\tThey are always part of the primary key
Data Modelling\tRelational DBMS\tIs optimized for analytical queries\tUses dimensional modeling\tIs based on two-dimensional relations\tDoes not use foreign keys\tIs based on two-dimensional relations
Data Modelling\tWhat are some of the key reasons data modeling\tTo increase data redundancy\tTo make systems more complex\tTo provide a common vocabulary around data\tTo avoid documentation\tTo provide a common vocabulary around data
Data Modelling\tList of Terms: Entity, Relationship, Attribute\tAre only used in relational models\tAre basic building blocks of most data models\tAre not relevant for NoSQL\tAre only physical concepts\tAre basic building blocks of most data models
Data Modelling\tWhat is NOT a goal of a data governance program\tEnsuring data quality\tMaximizing data redundancy\tEstablishing data standards\tPromoting data sharing\tMaximizing data redundancy
Data Modelling\tHow should a data model be maintained\tOnly when the system fails\tNever after initial deployment\tWhen requirements change\tOnly by DBAs\tWhen requirements change
Data Modelling\tWhich of these is NOT an activity for data modeling\tBuilding conceptual models\tReverse engineering databases\tWriting application code\tReviewing data models\tWriting application code
Data Modelling\Service Level Agreement Data Quality\tIs not measurable\tShould be ignored in data modeling\tDefines expected data quality levels\tOnly applies to transactional systems\tDefines expected data quality levels
Data Modelling\tHow does a composite key differ from a compound key\tA composite key includes non-key attributes\tThey are identical\tA compound key includes non-key attributes\tA composite key is always surrogate\tA composite key includes non-key attributes
Data Modelling\tWhen selecting a data modeling tool\tOnly cost matters\tFunctionality is irrelevant\tConsider support for forward and reverse engineering\tOpen source is always best\tConsider support for forward and reverse engineering
Data Modelling\tWhich of these statements is NOT true of designing a star schema\tDimensions are typically denormalized\tFact tables contain measurements\tSnowflaking is always required\tConformed dimensions enable sharing\tSnowflaking is always required
Data Modelling\tWhich of these is TRUE about defining high-quality data definitions\tThey should use obscure acronyms\tThey should be vague to allow flexibility\tThey should be clear, accurate, and complete\tThey should be technical only\tThey should be clear, accurate, and complete
Data Modelling\tWhat is one of the major differences between 3NF and star schema\t3NF is optimized for analytics\tStar schema is normalized\t3NF is denormalized\tStar schema uses dimensional modeling\tStar schema uses dimensional modeling
Data Modelling\tWhat is the role of the data modeler\tOnly to create physical models\tTo bridge business and technical requirements\tOnly to write SQL code\tOnly to manage databases\tTo bridge business and technical requirements
Data Modelling\tWhat is NOT a goal of data modeling\tCreating reusable data structures\tUnderstanding data requirements\tMaximizing data redundancy\tCommunicating data requirements\tMaximizing data redundancy
Data Modelling\tWhat are slowly changing dimensions\tDimensions that never change\tTechniques to manage changing dimension attributes\tFact tables that change slowly\tOnly used in normalized models\tTechniques to manage changing dimension attributes
Data Modelling\tWhat is the key difference between logical and physical data models\tLogical models are technology-specific\tPhysical models are more abstract\tLogical models are implementation-independent\tThere is no difference\tLogical models are implementation-independent
Data Modelling\tA data steward Business domain\tHas no role in data modeling\tIs only responsible for physical design\tEnsures business requirements are captured\tOnly works with NoSQL databases\tEnsures business requirements are captured
Data Modelling\tOne of the main approaches to managing personal data privacy\tIgnoring privacy concerns\tMaximizing data collection\tAnonymization and pseudonymization\tStoring everything indefinitely\tAnonymization and pseudonymization
Data Modelling\tAn area around the edge of an organisation … is known as\tCore business area\tData warehouse\tPeriphery\tExternal schema\tExternal schema
Data Modelling\tWhich of these principles are NOT part of the ISO 11179 standard\tNaming standards\tDefinition quality\tData redundancy\tMetadata registration\tData redundancy
Data Modelling\tThe 3 Vs … Which V are we referring to when talking about data accuracy\tVolume\tVelocity\tVariety\tVeracity\tVeracity
Data Modelling\tWhen creating an ethical data-handling strategy, which of these options is NOT a concern\tData minimization\tTransparency\tMaximizing data collection\tAccountability\tMaximizing data collection
Data Modelling\tAccording to the DMBOK, why is data modeling important\tTo increase system complexity\tTo reduce data quality\tTo provide a foundation for data governance\tTo avoid documentation\tTo provide a foundation for data governance
Data Modelling\tWhich of these is a core idea of data modeling according to Simsion and Hoberman\tData should be undocumented\tModels should be technology-specific\tUnderstanding how data fits together\tRedundancy is good\tUnderstanding how data fits together
Data Modelling\tData profiling assists in many aspects of improving data quality except\tIdentifying data anomalies\tValidating against metadata\tCreating redundant data\tExploring data content\tCreating redundant data
Data Modelling\tHow do data modeling tools typically support lineage\tBy storing definitions and mappings\tBy ignoring source systems\tBy only supporting physical models\tBy avoiding metadata\tBy storing definitions and mappings
Data Modelling\tThe highest level of these data-model types is the\tPhysical\tLogical\tConceptual\tCanonical\tConceptual
Data Modelling\tWhich of these is NOT a principle shared by organisations that get more value from their data\tData governance\tIgnoring data quality\tMaster data management\tData modeling standards\tIgnoring data quality
Data Modelling\tA relationship label that states 'each Student must attend at least one Course' represents\tOptional participation\tMandatory participation\tMany-to-many relationship\tNo relationship\tMandatory participation
Data Modelling\tThe design, implementation, and support of stored data, to maximize its value across its lifecycle is known as\tData modeling\tDatabase administration\tData architecture\tData governance\tData architecture
Data Modelling\tIn data modelling, what is a foreign key\tA key that opens databases\tAn attribute that uniquely identifies an entity\tAn attribute that references a primary key in another entity\tA type of index\tAn attribute that references a primary key in another entity
Data Integration and Interoperability\tWhich of the following is NOT part of the Extract, Transform, and Load (ETL) lifecycle process?\tLoad balancing\tExtract\tTransform\tLoad\tLoad
Data Integration and Interoperability\tThese are two metrics you must produce to track the effectiveness of your Data Integration and Interoperability program\tProfit and revenue\tData availability and latency\tEmployee satisfaction and turnover\tSystem uptime and downtime\tData availability and latency
Data Integration and Interoperability\tManaging Data Interoperability requires which of the following?\tAn enterprise perspective in design\tPoint-to-point interfaces only\tManual data entry\tDisregarding business rules\tAn enterprise perspective in design
Data Integration and Interoperability\tThe Extract, Transform, and Load (ETL) process\tMoves data only within a single database\tIs only relevant for batch processing\tIncludes Extract, Transform, and Load as essential steps\tIs outdated in modern architectures\tIncludes Extract, Transform, and Load as essential steps
Data Integration and Interoperability\tShould a Data Transformation Engine support unstructured data according to the DMBOK?\tOnly if the data is financial\tYes, if unstructured data needs to be accommodated\tNever, it is only for structured data\tOnly for real-time processing\tYes, if unstructured data needs to be accommodated
Data Integration and Interoperability\tWhich of the following is one of the ethical principles for Data Integration according to the DMBOK?\tMaximize data collection regardless of need\tEnsure business accountability for data design\tAvoid metadata documentation\tUse only open-source tools\tEnsure business accountability for data design
Data Integration and Interoperability\tWhat does a Metadata Repository store?\tOnly source code\tInformation about data structure, content, and business rules\tUser passwords only\tHardware inventory\tInformation about data structure, content, and business rules
Data Integration and Interoperability\tWhich of these is NOT a discipline mentioned as dependent on Data Integration and Interoperability?\tData Governance\tData Security\tData Modeling and Design\tData Visualization\tData Visualization
Data Integration and Interoperability\tWhich of these best describes the relationship between Data Integration and Big Data management?\tThey are unrelated\tData Integration is central to Big Data management\tBig Data replaces Data Integration\tData Integration only applies to relational databases\tData Integration is central to Big Data management
Data Integration and Interoperability\tWhen designing data integration solutions, ignoring specific error handling\tImproves performance\tIs considered best practice\tCan lead to data inconsistencies\tIs required for compliance\tCan lead to data inconsistencies
Data Integration and Interoperability\tWhich of these is the purpose of Data Integration and Interoperability?\tTo eliminate all data storage\tTo make data available in the format and timeframe needed by consumers\tTo increase IT complexity\tTo reduce the need for data governance\tTo make data available in the format and timeframe needed by consumers
Data Integration and Interoperability\tWhich of these metadata examples do NOT represent a transformation rule?\tGender code conversion from 0,1,2,3 to UNKNOWN,FEMALE,MALE,NOT PROVIDED\tData lineage tracking\tMapping source to target structures\tDe-duping rules\tData lineage tracking
Data Integration and Interoperability\tWhich of these data-quality issues is NOT prevented by Change Data Capture?\tDuplicate records\tIdentifying changed data\tReducing bandwidth usage\tPassing only deltas to targets\tDuplicate records
Data Integration and Interoperability\tWhich of these is NOT a common functional requirement for a data integration solution?\tData migration and conversion\tData consolidation into hubs\tManual data entry for all records\tManaging data interfaces\tManual data entry for all records
Data Integration and Interoperability\tWhat is NOT true about the ETL process?\tIt can be performed in batch or real-time\tIt always requires physical staging\tIt includes transformation steps\tIt can be implemented as ELT\tIt always requires physical staging
Data Integration and Interoperability\tEnterprise Service Bus (ESB)\tIs only used for batch processing\tActs as an intermediary between systems\tReplaces the need for APIs\tIs a type of database\tActs as an intermediary between systems
Data Integration and Interoperability\tWhat are some of the key reasons Data Integration and Interoperability is critical?\tTo increase data silos\tTo manage data movement efficiently across systems\tTo reduce the need for metadata\tTo avoid using vendor packages\tTo manage data movement efficiently across systems
Data Integration and Interoperability\tExtract, Transform, and Load, Enterprise Service Bus, Data Virtualization Server\tAre types of databases\tAre tools used in Data Integration\tAre programming languages\tAre data governance frameworks\tAre tools used in Data Integration
Data Integration and Interoperability\tWhat is NOT a goal of a Data Integration and Interoperability program?\tLower cost and complexity of managing solutions\tIncrease the number of point-to-point interfaces\tMake data available in needed formats\tSupport Business Intelligence efforts\tIncrease the number of point-to-point interfaces
Data Integration and Interoperability\tHow should a Data Sharing Agreement be developed?\tAfter data exchange begins\tWithout involving business stakeholders\tPrior to development of interfaces with MOU approved by data stewards\tOnly for external data sharing\tPrior to development of interfaces with MOU approved by data stewards
Data Integration and Interoperability\tWhich of these is NOT an activity for Data Integration and Interoperability?\tData discovery\tData profiling\tManual data deletion without governance\tDocumenting data lineage\tManual data deletion without governance
Data Integration and Interoperability\tData Sharing Agreement Master Data\tRequires no formal documentation\tShould specify responsibilities and acceptable use of exchanged data\tIs only for external organizations\tIs automatically generated by ETL tools\tShould specify responsibilities and acceptable use of exchanged data
Data Integration and Interoperability\tHow does Data Integration support Master Data Management?\tBy creating data silos\tBy transforming and integrating data from source systems to consolidated hubs\tBy eliminating the need for data governance\tBy reducing data quality requirements\tBy transforming and integrating data from source systems to consolidated hubs
Data Integration and Interoperability\tWhen selecting a Data Transformation Engine, which factor is NOT typically considered?\tSupport for batch and real-time functionality\tAbility to handle unstructured data\tColor of the user interface\tIntegration with existing tools\tColor of the user interface
Data Integration and Interoperability\tWhich of these statements is NOT true of designing a Canonical Model?\tIt reduces the number of data transformations needed\tIt requires agreement on a shared message format\tIt increases complexity with more than 100 systems\tIt can significantly lower support costs\tIt increases complexity with more than 100 systems
Data Integration and Interoperability\tWhich of these is TRUE about defining high-quality Data Exchange Standards?\tThey should be proprietary to each system\tThey should follow formal rules for data element structure\tThey should avoid industry standards\tThey should be developed without stakeholder input\tThey should follow formal rules for data element structure
Data Integration and Interoperability\tWhat is one of the major differences between Point-to-point and Hub-and-spoke interaction models?\tPoint-to-point is always faster\tHub-and-spoke requires a central data hub\tPoint-to-point scales better with many systems\tHub-and-spoke only works for real-time data\tHub-and-spoke requires a central data hub
Data Integration and Interoperability\tWhat is the role of the Business Rules Engine in DII?\tTo replace human decision-making entirely\tTo enable non-technical users to manage business rules\tTo slow down data processing\tTo eliminate the need for data governance\tTo enable non-technical users to manage business rules
Data Integration and Interoperability\tWhat is NOT a goal of Data Integration and Interoperability governance?\tEnsure business accountability for data design\tAllow purely technical approaches to data mapping\tSupport compliance with data regulations\tMaintain data lineage documentation\tAllow purely technical approaches to data mapping
Data Integration and Interoperability\tWhat are Staging Tables used for in ETL?\tPermanent data storage\tTemporary storage for transformed data before load\tReplacing target databases\tStoring user credentials\tTemporary storage for transformed data before load
Data Integration and Interoperability\tWhat is the key difference between ETL and ELT processes?\tELT loads data before transforming it\tETL is only for unstructured data\tELT never uses staging areas\tETL is faster than ELT for all scenarios\tELT loads data before transforming it
Data Integration and Interoperability\tA Data Steward in Data Integration\tShould never be involved in transformation rule design\tIs responsible for defining business rules for data transformation\tOnly manages hardware resources\tFocuses solely on application development\tIs responsible for defining business rules for data transformation
Data Integration and Interoperability\tOne of the main approaches to managing Streaming Data is\tUsing only batch processing\tMinimizing hardware investment\tUsing low latency solutions with in-memory processing\tStoring all data in traditional disk drives\tUsing low latency solutions with in-memory processing
Data Integration and Interoperability\tAn area around the edge of an organisation where data interfaces exist is known as?\tData lake\tEnterprise boundary\tCanonical model\tData mart\tEnterprise boundary
Data Integration and Interoperability\tWhich of these principles are NOT part of the Service-Oriented Architecture (SOA)?\tLoose coupling between services\tWell-defined service interfaces\tDirect database access by consumers\tService reusability\tDirect database access by consumers
Data Integration and Interoperability\tThe 3 Vs of Big Data - Which V are we referring to when talking about Velocity?\tThe speed of data processing and movement\tThe variety of data formats\tThe volume of data storage\tThe validity of data sources\tThe speed of data processing and movement
Data Integration and Interoperability\tWhen creating an ethical data-handling strategy, which of these options is NOT a concern?\tData privacy and security\tCompliance with regulations\tMaximizing data collection regardless of need\tTransparency in data usage\tMaximizing data collection regardless of need
Data Integration and Interoperability\tAccording to the DMBOK, why is Data Governance critical for DII?\tTo increase IT complexity\tTo govern transformation rules and message structures\tTo eliminate the need for metadata\tTo reduce business involvement\tTo govern transformation rules and message structures
Data Integration and Interoperability\tWhich of these is a core idea of Data Integration according to the DMBOK?\tData should remain in silos\tIntegration should only be technical\tBusiness experts should be involved in transformation rule design\tMetadata is optional\tBusiness experts should be involved in transformation rule design
Data Integration and Interoperability\tData profiling assists in many aspects of improving data quality except:\tIdentifying data structure issues\tDiscovering data inconsistencies\tReplacing the need for business rules\tAssessing data completeness\tReplacing the need for business rules
Data Integration and Interoperability\tHow do Enterprise Service Buses (ESB) facilitate data integration?\tBy creating tight coupling between systems\tBy acting as intermediaries for message exchange\tBy storing all data permanently\tBy eliminating the need for APIs\tBy acting as intermediaries for message exchange
Data Integration and Interoperability\tThe highest level of these data-model types is the:\tConceptual data model\tPhysical data model\tLogical data model\tStaging data model\tConceptual data model
Data Integration and Interoperability\tWhich of these is NOT a principle shared by organizations that get more value from their data?\tTaking an enterprise perspective\tBalancing local and enterprise needs\tIgnoring business accountability\tIterative and incremental delivery\tIgnoring business accountability
Data Integration and Interoperability\tA relationship label that states 'one-to-many' is an example of:\tA transformation rule\tA cardinality rule\tA business rule\tA data quality metric\tA cardinality rule
Data Integration and Interoperability\tThe design, implementation, and support of stored data, to maximize its value across its lifecycle is known as:\tData warehousing\tData governance\tData management\tData migration\tData management
Data Integration and Interoperability\tIn data modelling, what is a foreign key?\tA primary key in another table\tA key that opens physical locks\tA randomly generated number\tA data type definition\tA primary key in another table
Data Integration and Interoperability\tWhich of the following is NOT a benefit of using a Canonical Model?\tReduces number of transformations needed\tIncreases interface complexity\tLowers support costs\tStandardizes data exchange\tIncreases interface complexity
Data Integration and Interoperability\tWhat is the primary purpose of Change Data Capture (CDC)?\tTo capture all data regardless of changes\tTo reduce bandwidth by only capturing changed data\tTo increase data latency\tTo eliminate the need for ETL\tTo reduce bandwidth by only capturing changed data
Data Integration and Interoperability\tWhich latency type is characterized by data being processed in clumps or files on a schedule?\tReal-time synchronous\tBatch\tLow latency streaming\tEvent-driven\tBatch
Data Integration and Interoperability\tWhat is NOT a typical feature of Data-as-a-Service (DaaS)?\tData provided on demand\tData stored only locally\tLicensed from vendors\tUsed within organizations for data services\tData stored only locally
Data Integration and Interoperability\tWhich of these is NOT a common use case for Data Integration and Interoperability?\tData archiving\tData consolidation into hubs\tManual data entry without transformation\tManaging data interfaces\tManual data entry without transformation
Data Integration and Interoperability\tWhat does the 'T' in ETL stand for?\tTransfer\tTransform\tTransmit\tTranslate\tTransform
Data Integration and Interoperability\tWhich of these is NOT a type of interaction model mentioned in the text?\tStar schema\tPoint-to-point\tHub-and-spoke\tPublish-subscribe\tStar schema
Data Integration and Interoperability\tWhat is the main advantage of replication in data integration?\tIncreases data inconsistency\tMinimizes performance impact on primary systems\tRequires extensive programming\tOnly works with different database technologies\tMinimizes performance impact on primary systems
Data Integration and Interoperability\tWhich of these is NOT a component of process controls in data integration?\tDatabase activity logs\tAlerts\tException logs\tUser interface design\tUser interface design
Data Integration and Interoperability\tWhat is the purpose of data virtualization in DII?\tTo physically store all data in one location\tTo provide virtual access to distributed data stores\tTo eliminate the need for data transformation\tTo replace all databases\tTo provide virtual access to distributed data stores
Data Integration and Interoperability\tWhich of these is NOT mentioned as a business driver for DII?\tManaging data movement efficiently\tReducing support costs\tIncreasing application silos\tComplying with data standards\tIncreasing application silos
Data Integration and Interoperability\tWhat is the primary function of a Data Profiling Tool?\tTo create databases\tTo statistically analyze data sets for quality assessment\tTo write business rules\tTo manage user permissions\tTo statistically analyze data sets for quality assessment
Data Integration and Interoperability\tWhich of these is NOT a characteristic of asynchronous data integration?\tSending system waits for acknowledgment\tSystems can be offline independently\tLower latency than batch\tData updates measured in seconds or minutes\tSending system waits for acknowledgment
Data Integration and Interoperability\tWhat should be documented as part of data lineage in DII?\tOnly source systems\tOnly target systems\tHow data is acquired, moved, changed, and used\tOnly transformation rules\tHow data is acquired, moved, changed, and used
Data Integration and Interoperability\tWhich of these is NOT a goal of implementing DII practices?\tMake data available when needed\tIncrease complexity of interfaces\tConsolidate data into hubs\tSupport analytics efforts\tIncrease complexity of interfaces
Data Integration and Interoperability\tWhat is the relationship between Data Integration and Data Warehousing?\tThey are completely separate\tData Integration is critical to Data Warehousing\tData Warehousing replaces Data Integration\tOnly one is needed in an organization\tData Integration is critical to Data Warehousing
Data Integration and Interoperability\tWhich of these is NOT a type of data transformation mentioned?\tFormat changes\tSemantic conversion\tPhysical relocation of servers\tDe-duping\tPhysical relocation of servers
Data Integration and Interoperability\tWhat is the purpose of an Enterprise Application Integration (EAI) model?\tTo allow direct database access\tTo interact only through well-defined APIs\tTo eliminate all data stores\tTo increase system coupling\tTo interact only through well-defined APIs
Data Integration and Interoperability\tWhich of these metrics is NOT typically used to measure DII benefits?\tData availability\tSpeed of transmission\tEmployee lunch preferences\tSolution costs\tEmployee lunch preferences
Data Integration and Interoperability\tWhat is the main challenge with point-to-point integration as systems scale?\tToo few interfaces\tInterfaces approach s² complexity\tNo need for maintenance\tAlways faster than hub-and-spoke\tInterfaces approach s² complexity
Data Integration and Interoperability\tWhich of these is NOT a principle for implementing DII solutions?\tImplement through iterative delivery\tIgnore business needs\tBalance local and enterprise needs\tEnsure business accountability\tIgnore business needs
Data Integration and Interoperability\tWhat type of data integration has the lowest latency?\tBatch processing\tNear-real-time\tReal-time synchronous\tMicro-batch\tReal-time synchronous
Data Integration and Interoperability\tWhich of these is NOT a function of an Enterprise Service Bus?\tMessage routing\tService orchestration\tDirect database updates\tProtocol transformation\tDirect database updates
Data Integration and Interoperability\tWhat is the primary purpose of data archiving in DII?\tTo permanently delete old data\tTo move infrequently used data to less costly storage\tTo increase operational database size\tTo eliminate the need for backups\tTo move infrequently used data to less costly storage
Data Integration and Interoperability\tWhich of these is NOT a technique for Change Data Capture?\tTimestamp-based\tLog-based\tFull data refresh\tTrigger-based\tFull data refresh
Data Integration and Interoperability\tWhat is the benefit of using industry standards for data exchange?\tIncreases complexity\tSimplifies interoperability\tRequires more transformations\tIs only for government use\tSimplifies interoperability
Data Integration and Interoperability\tWhich of these is NOT a consideration when selecting data integration tools?\tSupport for batch and real-time\tHandling of unstructured data\tVendor's office location\tIntegration capabilities\tVendor's office location
Data Integration and Interoperability\tWhat is the purpose of the 'Load' step in ETL?\tTo extract data from sources\tTo transform data formats\tTo physically store or present data in target systems\tTo delete old data\tTo physically store or present data in target systems
Data Integration and Interoperability\tWhich of these is NOT a risk associated with batch processing?\tHigh latency\tTiming issues with business day processing\tAlways faster than real-time\tIncomplete data sets\tAlways faster than real-time
Data Integration and Interoperability\tWhat is the main characteristic of loosely coupled systems?\tOne system failure affects others\tSystems wait for responses from each other\tSystems can operate independently\tRequire synchronous communication\tSystems can operate independently
Data Integration and Interoperability\tWhich of these is NOT a benefit of cloud-based integration?\tScalability\tCost reduction\tVendor lock-in elimination\tFlexibility\tVendor lock-in elimination
Data Integration and Interoperability\tWhat should be the focus during data integration solution implementation?\tPurely technical aspects\tBusiness goals and requirements\tVendor tool features\tHardware specifications\tBusiness goals and requirements
Data Integration and Interoperability\tWhich of these is NOT a type of data latency mentioned?\tBatch\tReal-time synchronous\tHigh latency streaming\tAsynchronous\tHigh latency streaming
Data Integration and Interoperability\tWhat is the purpose of a Center of Excellence in DII?\tTo centralize all data storage\tTo specialize in enterprise DII design and deployment\tTo eliminate local teams\tTo increase vendor dependence\tTo specialize in enterprise DII design and deployment
Data Integration and Interoperability\tWhich of these is NOT a valid approach to data integration?\tPhysical integration\tVirtual integration\tNo integration needed\tHybrid integration\tNo integration needed
Data Integration and Interoperability\tWhat is the relationship between DII and Data Governance?\tDII replaces Data Governance\tData Governance governs DII transformation rules\tThey are completely separate\tData Governance only applies to storage\tData Governance governs DII transformation rules
Data Integration and Interoperability\tWhich of these is NOT a consideration in data mapping specifications?\tSource and target technical formats\tTransformation rules\tUser interface colors\tCalculation requirements\tUser interface colors
Data Integration and Interoperability\tWhat is the primary challenge with maintaining replicated data?\tToo much consistency\tRisk of unsynchronization when changes occur at multiple sites\tRequires no monitoring\tOnly works with identical databases\tRisk of unsynchronization when changes occur at multiple sites
Data Integration and Interoperability\tWhich of these is NOT a characteristic of service-oriented architecture (SOA)?\tTight coupling between services\tService reusability\tWell-defined interfaces\tService abstraction\tTight coupling between services
Data Integration and Interoperability\tWhat is the purpose of orchestration in data integration?\tTo eliminate all manual processes\tTo organize and execute multiple processes in order\tTo increase system complexity\tTo remove all controls\tTo organize and execute multiple processes in order
Data Integration and Interoperability\tWhich of these is NOT a type of data store mentioned in DII contexts?\tData warehouses\tData marts\tOperational data stores\tSocial media profiles\tSocial media profiles
Data Integration and Interoperability\tWhat is the main benefit of using micro-batch processing?\tHigher latency than daily batch\tLower latency than traditional batch\tMore complex than real-time\tRequires more storage\tLower latency than traditional batch
Data Integration and Interoperability\tWhich of these is NOT a function of Complex Event Processing (CEP)?\tIdentifying meaningful events\tPredicting behavior\tStoring all historical data permanently\tTriggering real-time responses\tStoring all historical data permanently
Data Integration and Interoperability\tWhat is the key consideration when implementing enterprise data integration solutions?\tFocus on replacing working solutions\tDesign for many systems, not just the first one\tOnly implement for single applications\tIgnore existing solutions\tDesign for many systems, not just the first one
Data Integration and Interoperability\tWhich of these is NOT a step in the data integration development lifecycle?\tPlanning\tDesign\tTesting\tMarketing\tMarketing
Data Integration and Interoperability\tWhat is the purpose of data discovery in DII?\tTo avoid understanding data sources\tTo identify potential data sources and assess quality\tTo eliminate metadata needs\tTo skip profiling\tTo identify potential data sources and assess quality
Data Integration and Interoperability\tWhich of these is NOT a type of transformation mentioned in ETL?\tRe-ordering data elements\tPhysical server relocation\tDe-duplicating records\tSemantic conversion\tPhysical server relocation
Data Integration and Interoperability\tWhat should trigger governance reviews in DII?\tOnly technical failures\tExceptions or critical events\tOnly business requests\tRandom schedules\tExceptions or critical events
Data Integration and Interoperability\tWhich of these is NOT a benefit of hub-and-spoke over point-to-point?\tReduces interface complexity\tProvides consistent data views\tIncreases the number of interfaces needed\tMinimizes source system impact\tIncreases the number of interfaces needed
Data Integration and Interoperability\tWhat is the main consideration for data retention in integration requirements?\tAll data must be kept forever\tRequirements differ by data domain and type\tOnly financial data needs retention\tRetention is not relevant\tRequirements differ by data domain and type
Data Integration and Interoperability\tWhich of these is NOT a function of data services in SOA?\tData retrieval\tData update\tDirect database manipulation\tData addition\tDirect database manipulation
Data Integration and Interoperability\tWhat is the relationship between data integration and compliance?\tDII makes compliance harder\tEnterprise DII enables re-use of compliance rules\tCompliance is not relevant to DII\tOnly manual processes support compliance\tEnterprise DII enables re-use of compliance rules
Data Integration and Interoperability\tWhich of these is NOT mentioned as a tool for DII?\tData transformation engine\tBusiness rules engine\tSpreadsheet software\tMetadata repository\tSpreadsheet software
Data Integration and Interoperability\tWhat is the primary challenge when integrating vendor packages?\tThey never need integration\tEach comes with its own data stores that must be integrated\tThey eliminate the need for data governance\tThey automatically provide canonical models\tEach comes with its own data stores that must be integrated
Data Integration and Interoperability\tWhich of these is NOT a consideration for real-time data integration?\tEvent triggering\tState management\tBatch job scheduling\tProcess dependencies\tBatch job scheduling
Data Integration and Interoperability\tWhat is the purpose of the publish-subscribe model in DII?\tTo create point-to-point connections\tTo ensure systems get consistent data sets from central publishing\tTo eliminate the need for data hubs\tTo increase interface complexity\tTo ensure systems get consistent data sets from central publishing
Data Integration and Interoperability\tWhich of these is NOT a benefit of standard tool implementations in DII?\tIncreased support costs\tImproved troubleshooting efficiency\tReduced staffing costs\tLower interface maintenance costs\tIncreased support costs
Data Integration and Interoperability\tWhat is the main consideration when designing data orchestration?\tOnly final data storage\tComplete flow from start to finish including dependencies\tOnly source system design\tOnly target system requirements\tComplete flow from start to finish including dependencies
Data Integration and Interoperability\tWhich of these is NOT a reason for data migration in DII?\tNew application implementation\tApplication retirement or merging\tData quality improvement\tTesting phases\tData quality improvement
Data Integration and Interoperability\tWhat is the relationship between DII and metadata?\tMetadata is optional for DII\tMetadata tracks technical inventory and business meaning\tDII eliminates the need for metadata\tMetadata only applies to storage\tMetadata tracks technical inventory and business meaning
Data Integration and Interoperability\tWhich of these is NOT a consideration when developing data services?\tMatching interaction model requirements\tCreating identical services for different needs\tUsing consistent tools\tReusing existing components\tCreating identical services for different needs
Reference and Master Data\tWhich of the following is NOT part of Master Data according to Chisholm's taxonomy?\tMetadata\tTransaction audit data\tReference Data\tEnterprise structure data\tTransaction activity data
Reference and Master Data\tThese are two metrics you must produce to track the effectiveness of your Reference and Master Data program\tData sharing volume and usage\tData Steward coverage\tData ingestion and consumption\tService Level Agreements\tTotal Cost of Ownership
Reference and Master Data\tManaging Reference Data\tRequires stewardship to ensure values are complete and current\tRequires entity resolution and identifier management\tRequires a System of Record and System of Reference\tRequires matching rules and cross-references\tRequires stewardship to ensure values are complete and current
Reference and Master Data\tThe Transaction Hub\tIs a system of record for Master Data\tIs a registry pointing to Master Data in source systems\tIs a consolidated repository for Master Data\tIs a data-sharing hub architecture\tIs a system of record for Master Data
Reference and Master Data\tShould Reference Data Management systems enforce use of standardized codes?\tYes, to ensure consistency across the organization\tNo, as it limits flexibility for business units\tOnly for industry-standard Reference Data\tOnly for proprietary Reference Data\tYes, to ensure consistency across the organization
Reference and Master Data\tWhich of the following is one of the ethical principles for Reference Data Management according to DMBOK?\tControlled change\tMaximizing data volume\tMinimizing data latency\tReducing data sharing\tControlled change
Reference and Master Data\tWhat does a System of Reference provide?\tReliable data for transactions and analysis\tThe authoritative source where data is created\tA registry pointing to Master Data\tA local data hub extension\tReliable data for transactions and analysis
Reference and Master Data\tWhich of these is NOT a Master Data entity type?\tParties\tProducts and Services\tFinancial structures\tGeographic positioning coordinates\tGeographic positioning coordinates
Reference and Master Data\tWhich of these best describes the relationship between Master Data and Reference Data?\tReference Data provides context for Master Data\tReference Data is a subset of Master Data\tMaster Data is less volatile than Reference Data\tReference Data requires entity resolution\tReference Data provides context for Master Data
Reference and Master Data\tWhen implementing Master Data Management, creating a Golden Record\tMay not be 100% accurate due to source discrepancies\tGuarantees 100% accuracy of all entity data\tEliminates the need for stewardship\tRequires no matching or merging processes\tMay not be 100% accurate due to source discrepancies
Reference and Master Data\tWhich of these is the purpose of Reference Data Management?\tEnsure complete and current values for each concept\tResolve entity instances across systems\tCreate a single version of truth\tManage transactional data\tEnsure complete and current values for each concept
Reference and Master Data\tWhich of these metadata examples do NOT represent Reference Data?\tCode value and description\tSource organization\tDefinition of a code\tEntity resolution rules\tEntity resolution rules
Reference and Master Data\tWhich of these data-quality issues is NOT prevented by Master Data Management?\tInconsistent data formats\tDuplicate entity instances\tAmbiguous identifiers\tIncomplete Reference Data\tIncomplete Reference Data
Reference and Master Data\tWhich of these is NOT a common functional requirement for a Master Data Management solution?\tEntity resolution\tData validation\tWorkflow automation\tGeographic data visualization\tGeographic data visualization
Reference and Master Data\tWhat is NOT true about Reference Data?\tIt requires entity resolution processes\tIt is less volatile than Master Data\tIt provides context for transactional data\tIt can include taxonomies and hierarchies\tIt requires entity resolution processes
Reference and Master Data\tRegistry Architecture\tPoints to Master Data in source systems\tIs the system of record for Master Data\tConsolidates data in a common repository\tRequires minimal changes to source systems\tPoints to Master Data in source systems
Reference and Master Data\tWhat are some of the key reasons Master Data Management fails?\tLack of proper governance\tOver-reliance on technical solutions\tInsufficient data profiling\tToo much standardization\tLack of proper governance
Reference and Master Data\tGolden Record, Trusted Source, System of Record\tAre all terms related to authoritative data\tAre all types of Reference Data\tAre all metadata attributes\tAre all architectural approaches\tAre all terms related to authoritative data
Reference and Master Data\tWhat is NOT a goal of a Reference Data Management program?\tEnable entity resolution across systems\tEnsure consistent values across processes\tLower data integration costs\tReduce complexity through standards\tEnable entity resolution across systems
Reference and Master Data\tHow should Reference Data changes be managed?\tThrough a defined process with approval and communication\tBy allowing individual business units to update as needed\tOnly when structural changes occur\tAutomatically without oversight\tThrough a defined process with approval and communication
Reference and Master Data\tWhich of these is NOT an activity for Master Data Management?\tDefining stewardship processes\tModeling Reference Data sets\tEstablishing governance policies\tEvaluating data sources\tModeling Reference Data sets
Reference and Master Data\tData Sharing Agreement\tStipulates what data can be shared and under what conditions\tIs only needed for external data sharing\tIs automatically enforced by MDM tools\tIs optional for internal data sharing\tStipulates what data can be shared and under what conditions
Reference and Master Data\tHow does Reference Data differ from Master Data?\tIt does not require entity resolution\tIt is more complex and volatile\tIt requires matching and merging\tIt focuses on transactional structure data\tIt does not require entity resolution
Reference and Master Data\tWhen selecting a Master Data Management tool\tConsider specialized MDM applications over generic tools\tAlways choose open-source solutions\tPrioritize cost over functionality\tFocus on single-domain solutions only\tConsider specialized MDM applications over generic tools
Reference and Master Data\tWhich of these statements is NOT true of designing a Reference Data Management system?\tIt should allow arbitrary changes without approval\tIt should enforce basic data entry rules\tIt should include workflows for approvals\tIt should support hierarchy maintenance\tIt should allow arbitrary changes without approval
Reference and Master Data\tWhich of these is TRUE about defining high-quality Reference Data?\tDefinitions must be clear and understandable\tCode values should be as generic as possible\tUpdates can be made without notification\tOnly industry standards should be used\tDefinitions must be clear and understandable
Reference and Master Data\tWhat is one of the major differences between Registry and Transaction Hub architectures?\tRegistry requires fewer changes to source systems\tTransaction Hub is easier to implement\tRegistry eliminates latency issues\tTransaction Hub is less costly\tRegistry requires fewer changes to source systems
Reference and Master Data\tWhat is the role of the Business Data Steward in Reference Data Management?\tEnsure data quality and approve changes\tPerform entity resolution algorithms\tCreate golden records\tManage system integrations\tEnsure data quality and approve changes
Reference and Master Data\tWhat is NOT a goal of Master Data Management?\tEnable arbitrary data changes\tEnsure accurate and current values\tReduce risks of ambiguous identifiers\tProvide consistent data across systems\tEnable arbitrary data changes
Reference and Master Data\tWhat are Cross-Reference Lists in Reference Data?\tTranslate between different code sets\tProvide hierarchical classifications\tStore only code values\tReplace entity resolution\tTranslate between different code sets
Reference and Master Data\tWhat is the key difference between Master Data and Transaction Data?\tMaster Data provides context for business activity\tTransaction Data is more volatile\tMaster Data records transaction details\tTransaction Data includes entity identifiers\tMaster Data provides context for business activity
Reference and Master Data\tA Data Steward in Master Data Management\tIs accountable for ensuring data quality\tPerforms system integrations\tDevelops MDM applications\tManages transactional data\tIs accountable for ensuring data quality
Reference and Master Data\tOne of the main approaches to managing sensitive Reference Data is\tEstablishing governance policies\tUsing only open-source tools\tAvoiding external data sources\tEliminating stewardship\tEstablishing governance policies
Reference and Master Data\tAn area around the edge of an organization that manages data locally is known as?\tA siloed business unit\tA system of record\tA transaction hub\tA consolidated repository\tA siloed business unit
Reference and Master Data\tWhich of these principles are NOT part of the Reference and Master Data Management guiding principles?\tLocal ownership by departments\tShared data across organization\tQuality monitoring and governance\tControlled change with oversight\tLocal ownership by departments
Reference and Master Data\tThe 3 Vs of big data - Which V are we referring to when talking about Reference Data completeness?\tVolume\tVelocity\tVariety\tVeracity\tVolume
Reference and Master Data\tWhen creating an ethical data-handling strategy, which of these options is NOT a concern?\tMaximizing data volume\tEnsuring data privacy\tMaintaining data security\tDefining retention policies\tMaximizing data volume
Reference and Master Data\tAccording to the DMBOK, why is Master Data Management challenging?\tPeople represent concepts differently and reconciliation is complex\tTools are too expensive\tData is always of high quality\tSystems are too integrated\tPeople represent concepts differently and reconciliation is complex
Reference and Master Data\tWhich of these is a core idea of Master Data Management according to Gartner?\tEnsuring uniformity and stewardship of shared assets\tFocusing on transactional data only\tAvoiding data governance\tUsing only open-source tools\tEnsuring uniformity and stewardship of shared assets
Reference and Master Data\tData profiling assists in many aspects of improving data quality except:\tPerforming entity resolution\tIdentifying data inconsistencies\tUnderstanding data structure\tAssessing data completeness\tPerforming entity resolution
Reference and Master Data\tHow do Business Data Stewards contribute to Master Data Management?\tBy resolving incorrectly matched records\tBy developing MDM applications\tBy performing system integrations\tBy creating transactional data\tBy resolving incorrectly matched records
Reference and Master Data\tThe highest level of these data-model types is the:\tCanonical model\tLogical data model\tPhysical data model\tSource system model\tCanonical model
Reference and Master Data\tWhich of these is NOT a principle shared by organizations that get more value from their data?\tMaintaining local control of data\tEnsuring data quality\tSharing data across functions\tUsing authoritative sources\tMaintaining local control of data
Reference and Master Data\tA relationship label that states 'one customer can have multiple addresses' represents:\tOne-to-many cardinality\tOne-to-one cardinality\tMany-to-many cardinality\tZero-to-many cardinality\tOne-to-many cardinality
Reference and Master Data\tThe design, implementation, and support of stored data to maximize its value across its lifecycle is known as:\tData management\tData integration\tData governance\tData modeling\tData management
Reference and Master Data\tIn data modeling, what is a foreign key?\tA field that links to a primary key in another table\tA unique identifier within a table\tA data quality metric\tA type of Reference Data\tA field that links to a primary key in another table
Metadata Management\tWhich of the following is NOT part of the Metadata lifecycle?\tMetadata storage\tMetadata usage\tMetadata deletion\tMetadata creation\tMetadata creation
Metadata Management\tThese are two metrics you must produce to track the effectiveness of your Metadata Management program\tMetadata repository uptime and query performance\tSteward representation and metadata usage\tData lake size and scan frequency\tETL job duration and SLA breaches\tSteward representation and metadata usage
Metadata Management\tManaging Metadata requires which of the following core disciplines?\tData governance\tNetwork engineering\tHardware procurement\tGraphic design\tData governance
Metadata Management\tThe Metadata strategy must align with:\tBusiness priorities\tVendor roadmaps only\tIndividual developer preferences\tLegacy system limitations\tBusiness priorities
Metadata Management\tShould a Metadata repository adhere to ISO/IEC 11179 standards?\tOnly if the data is relational\tNo, standards are optional\tYes, when exchanging data\tOnly for unstructured data\tYes, when exchanging data
Metadata Management\tWhich of the following is one of the ethical principles for handling business Metadata according to the DMBOK?\tMaximize storage regardless of sensitivity\tProtect sensitive information\tShare data with all stakeholders\tAvoid documentation\tProtect sensitive information
Metadata Management\tWhat does a Business Glossary primarily store?\tNetwork topology diagrams\tBusiness concepts and terminology\tETL job schedules\tDatabase passwords\tBusiness concepts and terminology
Metadata Management\tWhich of these is NOT a type of Metadata?\tOperational\tTechnical\tFinancial\tBusiness\tFinancial
Metadata Management\tWhich of these best describes the relationship between Metadata and data governance?\tMetadata is optional for governance\tData governance replaces Metadata\tMetadata is critical to data governance\tGovernance ignores Metadata\tMetadata is critical to data governance
Metadata Management\tWhen Metadata is outdated, which specific error is most likely to occur?\tIncreased confidence in data\tRedundant data and conflicting definitions\tFaster query performance\tHigher storage costs\tRedundant data and conflicting definitions
Metadata Management\tWhich of these is the purpose of Metadata lineage documentation?\tTo increase license fees\tTo obscure data sources\tTo trace data movement and transformation\tTo delete unused columns\tTo trace data movement and transformation
Metadata Management\tWhich of these metadata examples do NOT represent Business Metadata?\tData quality rules\tETL job details\tBusiness rules\tStakeholder contact information\tETL job details
Metadata Management\tWhich of these data-quality issues is NOT prevented by Metadata-driven validation rules?\tInvalid domain values\tMissing lineage documentation\tDuplicate records\tNon-existent primary keys\tMissing lineage documentation
Metadata Management\tWhich of these is NOT a common functional requirement for a Metadata repository?\tSearch and browse capability\tManual update interface\tGaming leaderboard\tVersioning support\tGaming leaderboard
Metadata Management\tWhat is NOT true about Metadata standards?\tThey simplify integration\tThey are always vendor-proprietary\tThey enable data exchange\tThey improve consistency\tThey are always vendor-proprietary
Metadata Management\tCentralized Metadata Architecture\tAlways eliminates the need for backups\tProvides a single repository of Metadata copies\tRemoves all need for governance\tForces real-time source queries\tProvides a single repository of Metadata copies
Metadata Management\tWhat are some of the key reasons Metadata governance processes should be formalized?\tTo increase licensing costs\tTo reduce data knowledge loss and ensure compliance\tTo discourage user feedback\tTo avoid documentation\tTo reduce data knowledge loss and ensure compliance
Metadata Management\tList of Terms: Business Metadata, Technical Metadata, Operational Metadata\tAdministrative Metadata only\tDescriptive, Structural, Administrative\tFinancial Metadata\tMarketing Metadata\tDescriptive, Structural, Administrative
Metadata Management\tWhat is NOT a goal of a Metadata Management program?\tEnsure Metadata quality and consistency\tIncrease data redundancy\tProvide standard access to Metadata\tDocument organizational knowledge\tIncrease data redundancy
Metadata Management\tHow should a Data Dictionary be treated to maximize reuse?\tAs a static printout\tAs a product requiring ongoing maintenance and stewardship\tAs a one-time project deliverable\tAs a marketing brochure\tAs a product requiring ongoing maintenance and stewardship
Metadata Management\tWhich of these is NOT an activity for Metadata integration?\tHarvesting from data models\tManual deletion of all logs\tScanning source systems\tLoading into a repository\tManual deletion of all logs
Metadata Management\tData Sharing Agreement Metadata\tMust ignore data sensitivity labels\tShould capture data sharing rules and agreements\tIs unnecessary for external feeds\tShould be deleted after ingestion\tShould capture data sharing rules and agreements
Metadata Management\tHow does Metadata tagging support Big Data ingestion?\tBy removing the need for storage\tBy enabling identification and governance of ingested data\tBy slowing down processing\tBy encrypting all files\tBy enabling identification and governance of ingested data
Metadata Management\tWhen selecting a Metadata repository tool, which factor is least important?\tSupport for open standards\tVendor’s gaming console compatibility\tIntegration capabilities\tScalability\tVendor’s gaming console compatibility
Metadata Management\tWhich of these statements is NOT true of designing a Metadata framework?\tIt should be aligned with enterprise subject areas\tIt must ignore historical versions\tIt should be generic and extensible\tIt should isolate users from disparate sources\tIt must ignore historical versions
Metadata Management\tWhich of these is TRUE about defining high-quality Metadata artefacts?\tDefinitions should be ambiguous\tTerms should be defined by using the term\tDefinitions must be exact and consistent\tDefinitions can contradict each other\tDefinitions must be exact and consistent
Metadata Management\tWhat is one of the major differences between centralized and distributed Metadata architectures?\tCentralized uses persistent storage while distributed queries sources in real time\tDistributed requires more gaming hardware\tCentralized always uses Excel\tDistributed never supports lineage\tCentralized uses persistent storage while distributed queries sources in real time
Metadata Management\tWhat is the role of the Data Steward in Metadata governance?\tTo ignore glossary updates\tTo manage the lifecycle of terms and definitions\tTo delete all historical versions\tTo avoid stakeholder contact\tTo manage the lifecycle of terms and definitions
Metadata Management\tWhat is NOT a goal of Metadata integration processes?\tConsolidate heterogeneous Metadata\tMaximize data redundancy\tEnsure lineage visibility\tSupport cross-source analysis\tMaximize data redundancy
Metadata Management\tWhat are Configuration Management Databases (CM DBs) within Metadata context?\tGaming scoreboards\tRepositories for IT asset Metadata and relationships\tSpreadsheets for vacation plans\tUnrelated to Metadata\tRepositories for IT asset Metadata and relationships
Metadata Management\tWhat is the key difference between 'As Designed' and 'As Implemented' data lineage?\tAs Designed' is theoretical mapping while 'As Implemented' reflects actual code execution\tThere is no difference\tAs Designed' is faster to query\tAs Implemented' is always wrong\tAs Designed' is theoretical mapping while 'As Implemented' reflects actual code execution
Metadata Management\tA Data Steward within a financial domain\tMust never document algorithms\tShould ensure regulatory compliance Metadata is accurate\tCan ignore data sensitivity labels\tShould avoid stakeholder contact\tShould ensure regulatory compliance Metadata is accurate
Metadata Management\tOne of the main approaches to managing sensitive Metadata is\tPublicly exposing all data\tTagging and classifying sensitive data\tDeleting all logs\tIgnoring encryption\tTagging and classifying sensitive data
Metadata Management\tAn area around the edge of an organisation that exchanges data with external partners is known as?\tData swamp\tPerimeter data zone\tDMZ (Demilitarized Zone)\tGaming arena\tPerimeter data zone
Metadata Management\tWhich of these principles are NOT part of the ISO/IEC 11179 Metadata Registry Standard?\tNaming and identification principles\tData deletion mandates\tRegistration of data elements\tRules for data definitions\tData deletion mandates
Metadata Management\tThe 3 Vs of Big Data: Which V are we referring to when talking about Metadata completeness?\tVelocity\tVariety\tVolume\tVeracity\tVeracity
Metadata Management\tWhen creating an ethical data-handling strategy, which of these options is NOT a concern?\tData sensitivity classification\tRegulatory compliance\tMaximizing ad revenue regardless of privacy\tTransparent consent mechanisms\tMaximizing ad revenue regardless of privacy
Metadata Management\tAccording to the DMBOK, why is Metadata critical to data governance?\tIt replaces all human roles\tIt explains data and processes enabling governance\tIt slows down development\tIt increases licensing costs\tIt explains data and processes enabling governance
Metadata Management\tWhich of these is a core idea of Metadata Management according to Loshin and Aiken?\tHarvesting definitions without validation\tIntentional development of definitions with stewardship\tIgnoring existing dictionaries\tBypassing governance\tIntentional development of definitions with stewardship
Metadata Management\tData profiling assists in many aspects of improving data quality except:\tIdentifying data domains\tCreating intentional Metadata\tReplacing the need for governance\tDetecting anomalies\tReplacing the need for governance
Metadata Management\tHow do Data Integration Tools contribute to Metadata lineage?\tBy deleting all logs\tBy documenting source-to-target mappings and transformations\tBy avoiding version control\tBy encrypting passwords only\tBy documenting source-to-target mappings and transformations
Metadata Management\tThe highest level of these data-model types is the:\tPhysical model\tConceptual model\tNetwork diagram\tGaming schema\tConceptual model
Metadata Management\tWhich of these is NOT a principle shared by organisations that get more value from their data?\tConsistent Metadata standards\tIgnoring Metadata quality\tGovernance oversight\tStewardship accountability\tIgnoring Metadata quality
Metadata Management\tA relationship label that states ‘each customer may have zero or more orders’ represents which cardinality rule?\tOne-to-one mandatory\tOne-to-many optional\tMany-to-many mandatory\tOne-to-one optional\tOne-to-many optional
Metadata Management\tThe design, implementation, and support of stored data, to maximize its value across its lifecycle is known as:\tData mining\tMetadata Management\tGame development\tNetwork administration\tMetadata Management
Metadata Management\tIn data modelling, what is a foreign key?\tA key used for gaming encryption\tA column that references a primary key in another table\tA physical lock on a server\tA backup password\tA column that references a primary key in another table
Data Quality\tWhich of the following is NOT part of the Data Quality Improvement Lifecycle\tData Governance activities\tRoot cause remediation\tPrevention\tPlan-do-check-act cycle\tPlan-do-check-act cycle
Data Quality\tThese are two metrics you must produce to track the effectiveness of your Data Quality program\tData issue counts and issue resolution time\tData completeness and uniqueness\tReturn on Investment and quality trend over time\tNumber of data stewards and SLA penalties\tReturn on Investment and quality trend over time
Data Quality\tManaging Data Quality requires\tOnly technical staff involvement\tCross-functional commitment and coordination\tSenior management approval once\tExternal vendor support\tCross-functional commitment and coordination
Data Quality\tThe Shewhart/Deming cycle\tFocuses solely on technology fixes\tIs a one-time project phase\tIs a continuous improvement loop based on plan-do-check-act\tMeasures only data completeness\tIs a continuous improvement loop based on plan-do-check-act
Data Quality\tShould Metadata Repositories contain Data Quality rules and measurement results\tYes\tNo\tOnly for Master Data\tOnly if ISO 8000 is adopted\tYes
Data Quality\tWhich of the following is one of the ethical principles for data handling according to the Leader’s Data Manifesto\tMaximize data collection\tInvolvement from people at all levels\tLimit transparency\tAvoid governance\tInvolvement from people at all levels
Data Quality\tWhat does a Data Quality Business Rule describe\tOrganizational hierarchy\tHow data should exist to be useful and usable\tStorage capacity limits\tSoftware licensing terms\tHow data should exist to be useful and usable
Data Quality\tWhich of these is NOT a Data Quality dimension in the DAMA UK white paper\tCompleteness\tUniqueness\tProfitability\tValidity\tProfitability
Data Quality\tWhich of these best describes the relationship between Data Quality and Metadata\tMetadata is optional for Data Quality\tMetadata defines expectations against which quality is measured\tMetadata only affects storage costs\tThey are unrelated\tMetadata defines expectations against which quality is measured
Data Quality\tWhen business rule changes are not propagated throughout the entire system\tData accuracy improves automatically\tData errors will result\tProcessing speed increases\tStorage costs drop\tData errors will result
Data Quality\tWhich of these is the purpose of Data Cleansing\tIncrease storage costs\tTransform data to conform to standards and correct errors\tCreate redundant copies\tBypass governance\tTransform data to conform to standards and correct errors
Data Quality\tWhich of these metadata examples do NOT represent a Data Quality requirement\tState code must be a valid USPS abbreviation\tCustomer email must be populated for 98% of records\tServer CPU utilization\tZIP Code must match the State\tServer CPU utilization
Data Quality\tWhich of these data-quality issues is NOT prevented by enforcing referential integrity\tDuplicate data breaking uniqueness\tOrphan rows causing inconsistent reports\tDefault values replacing missing data\tStale business rules\tStale business rules
Data Quality\tWhich of these is NOT a common functional requirement for a Data Quality tool\tData profiling capabilities\tAutomated rule monitoring\tVideo editing features\tIssue tracking workflow\tVideo editing features
Data Quality\tWhat is NOT true about the Data Quality Improvement Lifecycle\tIt includes a Plan stage\tIt requires root cause analysis\tIt is a one-time project\tIt uses the Shewhart/Deming cycle\tIt is a one-time project
Data Quality\tISO 8000\tDefines data portability standards only\tCovers data quality planning, control, assurance, and improvement\tIs limited to Master Data syntax\tReplaces all other data standards\tCovers data quality planning, control, assurance, and improvement
Data Quality\tWhat are some of the key reasons Data Quality Management\tTo maximize storage capacity\tTo reduce risks and costs associated with poor quality data\tTo increase software licensing fees\tTo avoid regulatory compliance\tTo reduce risks and costs associated with poor quality data
Data Quality\tCritical Data, Master Data, Regulatory Reporting, Financial Reporting\tAre examples of data value domains\tAre common drivers for assessing data criticality\tDefine ETL transformation rules\tSpecify SLA penalties\tAre common drivers for assessing data criticality
Data Quality\tWhat is NOT a goal of a Data Quality program\tImprove organizational efficiency\tSet measurable data quality standards\tIncrease data redundancy\tProtect organizational reputation\tIncrease data redundancy
Data Quality\tHow should Data Quality rules be documented\tIn isolated spreadsheets\tConsistently using established standards and templates\tOnly in source code comments\tOnly by senior management\tConsistently using established standards and templates
Data Quality\tWhich of these is NOT an activity for Data Quality Improvement\tPerforming root cause analysis\tImplementing monitoring controls\tIgnoring stakeholder feedback\tPrioritizing remediation\tIgnoring stakeholder feedback
Data Quality\tService Level Agreement Data Quality thresholds\tAre set arbitrarily by IT\tAre defined by business clients based on acceptability\tAre fixed at 100% for all data\tAre never escalated\tAre defined by business clients based on acceptability
Data Quality\tHow does Data Profiling assist Data Quality\tIt replaces the need for business rules\tIt only counts rows in tables\tIt helps discover structure, content, and quality issues\tIt ensures 100% accuracy automatically\tIt helps discover structure, content, and quality issues
Data Quality\tWhen selecting a Data Quality tool\tChoose the cheapest regardless of fit\tFocus only on vendor reputation\tEvaluate fit with organizational rules and processes\tIgnore integration requirements\tEvaluate fit with organizational rules and processes
Data Quality\tWhich of these statements is NOT true of designing a Data Quality SLA\tIt should specify escalation strategies\tIt should define roles and responsibilities\tIt should ignore business impact\tIt should set timelines for resolution\tIt should ignore business impact
Data Quality\tWhich of these is TRUE about defining high-quality data\tIt is the same for every organization\tIt must be fit for the purposes of data consumers\tIt is determined solely by IT\tIt focuses only on accuracy\tIt must be fit for the purposes of data consumers
Data Quality\tWhat is one of the major differences between Data Cleansing and Data Enhancement\tCleansing removes data while enhancement deletes metadata\tCleansing corrects errors whereas enhancement adds attributes to improve usability\tEnhancement reduces data volume\tCleansing is manual only\tCleansing corrects errors whereas enhancement adds attributes to improve usability
Data Quality\tWhat is the role of the Data Quality program team\tTo fix all data issues alone\tTo engage business and technical staff and drive quality techniques\tTo replace Data Governance\tTo avoid stakeholder involvement\tTo engage business and technical staff and drive quality techniques
Data Quality\tWhat is NOT a goal of Data Quality Management\tPrevent data errors\tCorrect symptoms only\tMeasure quality objectively\tEmbed standards in business processes\tCorrect symptoms only
Data Quality\tWhat are Data Quality Dimensions\tSoftware modules for ETL\tMeasurable features or characteristics of data used to define quality\tUser interface controls\tStorage compression algorithms\tMeasurable features or characteristics of data used to define quality
Data Quality\tWhat is the key difference between Data Quality and Data Cleansing\tData Quality is a process while Cleansing is a dimension\tData Quality focuses on prevention whereas Cleansing is corrective action\tData Quality is only for Master Data\tCleansing prevents all future errors\tData Quality focuses on prevention whereas Cleansing is corrective action
Data Quality\tA Data Steward in the context of Data Quality\tIs solely responsible for fixing all data errors\tWorks with DQ analysts to confirm rules and prioritize issues\tOnly handles storage optimization\tDefines software licenses\tWorks with DQ analysts to confirm rules and prioritize issues
Data Quality\tOne of the main approaches to managing Sensitive Data quality issues\tIgnoring encryption\tManual patches without governance\tImplementing preventive controls and governance\tDeleting all sensitive data\tImplementing preventive controls and governance
Data Quality\tAn area around the edge of an organization where data quality issues often originate is known as\tData Lake\tData Swamp\tPerimeter systems or external data feeds\tMaster Data Hub\tPerimeter systems or external data feeds
Data Quality\tWhich of these principles are NOT part of the ISO 8000 data quality standard\tPortability\tMeeting stated requirements\tProvenance tracking\tMaximum redundancy\tMaximum redundancy
Data Quality\tThe 3 Vs of big data – Which V are we referring to when talking about data Veracity\tVolume\tVelocity\tVeracity\tVariety\tVeracity
Data Quality\tWhen creating an ethical data-handling strategy, which of these options is NOT a concern\tTransparency\tAccountability\tMaximizing data hoarding\tFairness\tMaximizing data hoarding
Data Quality\tAccording to the DMBOK, why is Data Quality Management essential\tTo increase server costs\tBecause poor quality data increases risks and costs\tTo avoid stakeholder engagement\tTo eliminate all software bugs\tBecause poor quality data increases risks and costs
Data Quality\tWhich of these is a core idea of Data Quality shared by Strong-Wang, Redman, and English\tOnly accuracy matters\tDimensions provide measurable characteristics to define quality\tData should never be shared\tTools replace governance\tDimensions provide measurable characteristics to define quality
Data Quality\tData profiling assists in many aspects of improving data quality except\tIdentifying null counts\tDiscovering hidden business rules\tAutomatically fixing all root causes\tFinding format non-conformance\tAutomatically fixing all root causes
Data Quality\tHow do Data Quality rules support Statistical Process Control\tThey replace control charts\tThey define measurable standards against which variation is assessed\tThey eliminate common causes\tThey increase data volume\tThey define measurable standards against which variation is assessed
Data Quality\tThe highest level of these data-model types is the\tConceptual model\tPhysical schema\tETL script\tDatabase index\tConceptual model
Data Quality\tWhich of these is NOT a principle shared by organizations that get more value from their data\tEmbedding quality in business processes\tRelying on manual patches for fixes\tEstablishing governance and standards\tMeasuring quality objectively\tRelying on manual patches for fixes
Data Quality\tA relationship label that states ‘Each customer must have one and only one email address’ enforces\tReferential integrity\tCardinality rule of one-to-one\tData enhancement\tTemporal mismatch\tCardinality rule of one-to-one
Data Quality\tThe design, implementation, and support of stored data, to maximize its value across its lifecycle is known as\tData entry\tData architecture management\tRandom patching\tSoftware licensing\tData architecture management
Data Quality\tIn data modelling, what is a foreign key\tA unique identifier for a table\tA column that references the primary key of another table\tA data quality dimension\tAn encryption key\tA column that references the primary key of another table
Data Quality\tWhich of these is NOT a barrier to effective Data Quality Management\tLack of leadership support\tDifficulty justifying improvements\tHigh organizational awareness\tInappropriate instruments to measure value\tHigh organizational awareness
Data Quality\tWhich of these is TRUE about Data Quality reporting\tIt should focus only on technical metrics\tIt should align with SLA metrics and highlight business impact\tIt should be shared only with IT\tIt should avoid trend analysis\tIt should align with SLA metrics and highlight business impact
Data Quality\tHow should Data Quality issues be escalated\tRandomly to any available staff\tAccording to a defined escalation strategy in the SLA\tOnly via email to senior management\tBy ignoring low-impact issues\tAccording to a defined escalation strategy in the SLA
Data Quality\tWhich of these statements is NOT true of Data Quality preventive actions\tThey stop known errors from occurring\tThey include training and enforcing rules\tThey eliminate the need for any monitoring\tThey build quality into processes\tThey eliminate the need for any monitoring
Data Quality\tWhat is NOT a characteristic of effective Data Quality metrics\tBusiness relevance\tUnlimited scope\tMeasurability\tControllability\tUnlimited scope
Data Quality\tWhen should data quality requirements be defined\tAfter deployment when issues arise\tDuring the Plan stage before system build\tOnly during audits\tNever, they emerge naturally\tDuring the Plan stage before system build
Data Quality\tWhich of these is a recommended corrective action for data quality issues\tManual patches without testing\tSolving the problem at its root cause\tIgnoring non-technical causes\tOnly fixing data downstream\tSolving the problem at its root cause
Data Quality\tWhich of these is NOT a technique for root cause analysis\tFive Whys\tPareto analysis\tFishbone diagram\tRandom guessing\tRandom guessing
Data Quality\tWhat is a primary output of Data Quality profiling\tA fully cleansed data set\tStatistics and patterns to identify potential quality issues\tFinal business rules\tSecurity certificates\tStatistics and patterns to identify potential quality issues
Data Quality\tWhich of these best describes the role of Data Governance in Data Quality\tIt replaces the need for a DQ team\tIt provides policies, standards, and coordination to support quality\tIt focuses only on storage optimization\tIt eliminates the need for metrics\tIt provides policies, standards, and coordination to support quality
Data Quality\tWhich of these is a common type of Data Quality business rule\tColor scheme for dashboards\tFormat compliance for telephone numbers\tServer uptime threshold\tMarketing campaign budget\tFormat compliance for telephone numbers
Data Quality\tWhat does the ISO 8000 standard emphasize about data requirements\tThey should be vague and flexible\tThey must be stated clearly and unambiguously\tThey are optional for portability\tThey only apply to Master Data\tThey must be stated clearly and unambiguously
Data Quality\tWhich of these is NOT an example of data enhancement\tAdding demographic attributes to customer records\tGeocoding addresses\tDeleting null records\tTimestamping data events\tDeleting null records
Data Quality\tHow can Statistical Process Control be applied to Data Quality\tBy ignoring variation in data processes\tBy using control charts to detect special causes in data measurements\tBy focusing solely on data volume\tBy eliminating the need for rules\tBy using control charts to detect special causes in data measurements
Data Quality\tWhich of these is a sign of a mature Data Management Organization\tRetrofitting quality after deployment\tDefining data quality criteria at the beginning of a process\tRelying on manual patches\tAvoiding governance\tDefining data quality criteria at the beginning of a process
Data Quality\tWhich of these is NOT recommended when performing manual data correction\tUsing an interface with controls and edits\tCreating an audit trail\tMaking direct updates in production without oversight\tDocumenting changes\tMaking direct updates in production without oversight
Data Quality\tWhich of these is TRUE about Data Quality and organizational culture\tCulture change is unnecessary if tools are good\tCultural change is required to embed quality mindset across the organization\tOnly IT needs to change behavior\tQuality is solely a technical responsibility\tCultural change is required to embed quality mindset across the organization
Data Quality\tWhich of these is a typical responsibility of a Data Quality SLA\tDefine software licensing terms\tSpecify acceptable quality thresholds per data element\tSet marketing budgets\tApprove hardware purchases\tSpecify acceptable quality thresholds per data element
Data Quality\tWhich of these is NOT a benefit of high-quality data as stated in the chapter\tIncreased customer service costs\tImproved organizational efficiency\tReduced risk of fines\tEnhanced reputation\tIncreased customer service costs
Data Quality\tWhat is the primary purpose of Data Quality incident tracking\tTo assign blame\tTo document issues, root causes, and resolutions for continuous improvement\tTo hide problems from stakeholders\tTo increase paperwork\tTo document issues, root causes, and resolutions for continuous improvement
Data Quality\tWhich of these is NOT a step in the initial Data Quality assessment\tDefine goals and scope\tInspect data against rules\tIgnore stakeholder confirmation\tDocument non-conformance\tIgnore stakeholder confirmation
Data Quality\tWhich of these best captures the principle of ‘Criticality’ in Data Quality\tAll data must meet 100% quality\tFocus improvement on data most critical to enterprise and customers\tOnly Master Data matters\tAvoid measuring non-critical data\tFocus improvement on data most critical to enterprise and customers
Data Quality\tWhich of these is a key output of the ‘Do’ stage in the Data Quality Improvement Lifecycle\tFinalized SLA penalties\tImplementation of root cause fixes and monitoring plans\tComplete elimination of future issues\tDisbanding the DQ team\tImplementation of root cause fixes and monitoring plans
Data Quality\tWhich of these is NOT a reason for establishing a formal Data Quality program according to business drivers\tIncrease the value of organizational data\tReduce risks and costs\tDecrease productivity\tProtect reputation\tDecrease productivity
Data Quality\tWhich of these is TRUE regarding Data Quality and Data Governance policies\tThey conflict with each other\tQuality policies should support and be supported by governance\tGovernance policies are optional for quality\tOnly quality needs policies\tQuality policies should support and be supported by governance
Data Quality\tWhich of these tools is primarily used for ongoing monitoring after initial data profiling\tVideo conferencing software\tData querying tools for deeper analysis and pattern discovery\tSpreadsheet for manual counts\tWord processor for documentation\tData querying tools for deeper analysis and pattern discovery
Data Quality\tWhich of these is an example of a Mapping conformance business rule\tEnsuring all dates are in MM/DD/YYYY format\tValidating that ‘AL’ and ‘01’ both map to ‘Alabama’\tChecking that age is greater than zero\tCounting null values in a column\tValidating that ‘AL’ and ‘01’ both map to ‘Alabama'
Data Quality\tWhich of these is NOT a common timing issue that affects data quality\tData delivered late impacting downstream quality\tStable yet responsive to legitimate change requests\tImmediate real-time updates only\tSeasonal process delays\tImmediate real-time updates only
Data Quality\tWhich of these best describes the concept of Data Quality ‘embedded in business processes’\tQuality checks are performed only at year-end\tBusiness process owners enforce quality standards within their workflows\tIT fixes issues after each quarter\tExternal auditors handle all quality\tBusiness process owners enforce quality standards within their workflows
Data Quality\tWhich of these is a recommended practice when developing Data Quality metrics\tMeasure everything possible\tEnsure metrics are controllable and tied to business impact\tFocus on technical elegance only\tAvoid stakeholder approval\tEnsure metrics are controllable and tied to business impact
Data Quality\tWhich of these is NOT a typical root cause category identified during Data Quality analysis\tProcess delays\tSystem design flaws\tUser training gaps\tRandom cosmic rays\tRandom cosmic rays
Data Quality\tWhich of these best explains why building quality into processes from the beginning is preferred\tIt is more expensive upfront\tIt costs less than retrofitting and reduces risk\tIt delays project delivery\tIt eliminates the need for governance\tIt costs less than retrofitting and reduces risk
Data Quality\tWhich of these is a key characteristic of reusable quality check code modules\tThey increase development time\tThey ensure consistent execution and simplify maintenance\tThey only work for one project\tThey bypass audits\tThey ensure consistent execution and simplify maintenance
Data Quality\tWhich of these is NOT mentioned as a cause of data quality issues in data entry processes\tPoorly designed interfaces\tField overloading\tExcessive governance controls\tTraining issues\tExcessive governance controls
Data Quality\tWhich of these is a primary focus of Data Quality Assurance within ISO 8000 Part 61\tWriting marketing slogans\tEnsuring processes meet data quality standards\tIncreasing hardware sales\tCreating software licenses\tEnsuring processes meet data quality standards
Data Quality\tWhich of these best describes the impact of poor-quality data on customer experience\tIt always improves loyalty\tIt can lead to wrong products, complaints, and reputational damage\tIt reduces call center workload\tIt has no measurable impact\tIt can lead to wrong products, complaints, and reputational damage
Data Quality\tWhich of these is a recommended way to standardize data values\tRandom manual edits\tPattern-based parsing and transformation into canonical formats\tIgnoring format variations\tUsing proprietary formats only\tPattern-based parsing and transformation into canonical formats
Data Quality\tWhich of these is NOT a principle of Data Quality programs listed in the chapter\tPrevention\tRoot cause remediation\tMaximizing redundancy\tStandards-driven\tMaximizing redundancy
Data Quality\tWhich of these is a typical outcome of effective Data Quality training for employees\tIncreased data errors\tBetter understanding of data impacts and responsibilities\tAvoidance of all governance\tElimination of all tools\tBetter understanding of data impacts and responsibilities
Data Quality\tWhich of these is NOT a component of the ISO 8000 view of quality data\tPortable data\tMeets stated requirements\tTied to a specific licensed software\tConforms to specifications\tTied to a specific licensed software
Data Quality\tWhich of these best describes the relationship between Data Quality rules and Service Level Agreements\tRules are optional within SLAs\tRules define measurable expectations that SLAs monitor and enforce\tSLAs replace the need for rules\tThey operate independently\tRules define measurable expectations that SLAs monitor and enforce
Data Quality\tWhich of these is a common pitfall when fixing data quality issues\tAddressing root causes\tImplementing preventive controls\tApplying untested manual patches directly in production\tEngaging stakeholders\tApplying untested manual patches directly in production
Data Quality\tWhich of these is NOT a purpose of Data Quality reporting\tCommunicate quality levels to stakeholders\tTrack trends and effectiveness of improvements\tHide issues from management\tSupport data-driven decisions\tHide issues from management
Data Quality\tWhich of these best describes the concept of ‘Objective measurement and transparency’ in Data Quality\tMeasurements are kept secret\tQuality levels are measured objectively and shared with stakeholders\tOnly IT sees the metrics\tSubjective opinions guide all decisions\tQuality levels are measured objectively and shared with stakeholders
Data Warehouse and Business Intelligence\tWhich of the following is NOT one of the primary goals of implementing a data warehouse?\tSupport Business Intelligence activity\tEnable effective business analysis and decision-making\tFind ways to innovate based on insights from their data\tReplace all operational systems\tEnable effective business analysis and decision-making
Data Warehouse and Business Intelligence\tThese are two metrics you must produce to track the effectiveness of your Business Intelligence program\tAverage query response time and number of users per day\tNumber of registered users and subject area coverage percentages\tResponse time and data retention\tNumber of licensed tools and data source governance\tAverage query response time and number of users per day
Data Warehouse and Business Intelligence\tWhich of the following is NOT a guiding principle for data warehouse implementation?\tFocus on business goals\tStart with the end in mind\tSummarize and optimize first\tPromote transparency and self-service\tPromote transparency and self-service
Data Warehouse and Business Intelligence\tThe Kimball approach to data warehousing\tUses dimensional modeling with star schemas\tUses normalized relational models\tFocuses on real-time data integration\tEmphasizes operational systems\tUses dimensional modeling with star schemas
Data Warehouse and Business Intelligence\tShould a Data Warehouse replace all operational reporting systems?\tYes, immediately\tOnly for regulatory compliance\tNo, it should complement them\tOnly for historical data\tNo, it should complement them
Data Warehouse and Business Intelligence\tWhich of the following is one of the ethical principles for handling sensitive data in a data warehouse?\tData discrimination based on risk exposure\tMaximize data collection\tShare all data freely\tIgnore data lineage\tData discrimination based on risk exposure
Data Warehouse and Business Intelligence\tWhat does a staging area in a data warehouse environment do?\tStores transformed data for end users\tProvides real-time analytics\tServes as an intermediate data store for transformation\tReplaces the need for data marts\tServes as an intermediate data store for transformation
Data Warehouse and Business Intelligence\tWhich of these is NOT a component of Inmon's Corporate Information Factory?\tStaging Area\tOperational Data Store\tData Lake\tData marts\tData Lake
Data Warehouse and Business Intelligence\tWhich of these best describes the relationship between a Data Warehouse and Business Intelligence tools?\tBI tools replace the need for a data warehouse\tA data warehouse provides the data that BI tools analyze\tThey are completely independent systems\tBI tools generate data for the warehouse\tA data warehouse provides the data that BI tools analyze
Data Warehouse and Business Intelligence\tWhen designing a data warehouse, failing to start with business goals results in\tBetter technical architecture\tMisalignment with organizational priorities\tFaster implementation\tLower costs\tMisalignment with organizational priorities
Data Warehouse and Business Intelligence\tWhich of these is the purpose of a Data Mart?\tStore operational transaction data\tProvide real-time data feeds\tSupport specific analytical needs of a department\tReplace the data warehouse\tSupport specific analytical needs of a department
Data Warehouse and Business Intelligence\tWhich of these metadata examples do NOT represent business context?\tData types and structure\tBusiness rules for calculations\tData source descriptions\tSecurity restrictions\tData types and structure
Data Warehouse and Business Intelligence\tWhich of these data-quality issues is NOT prevented by implementing data profiling?\tInconsistent data formats\tMissing values\tDuplicate records\tPoor user interface design\tPoor user interface design
Data Warehouse and Business Intelligence\tWhich of these is NOT a common functional requirement for a Business Intelligence solution?\tData visualization capabilities\tReal-time data feeds for all users\tUser access controls\tPerformance optimization\tReal-time data feeds for all users
Data Warehouse and Business Intelligence\tWhat is NOT true about the Kimball approach to data warehousing?\tIt uses dimensional modeling\tIt focuses on departmental data marts\tIt requires an enterprise data model\tIt uses conformed dimensions\tIt requires an enterprise data model
Data Warehouse and Business Intelligence\tData Lake\tIs the same as a data warehouse\tOnly stores structured data\tIngests data before integrating it\tProvides real-time operational reporting\tIngests data before integrating it
Data Warehouse and Business Intelligence\tWhat are some of the key reasons data governance is important for a data warehouse?\tTo curtail all data usage\tTo ensure compliance and data quality\tTo reduce storage costs\tTo eliminate metadata\tTo ensure compliance and data quality
Data Warehouse and Business Intelligence\tList of Terms: Which of these is NOT part of the 3 Vs of big data?\tVolume\tVelocity\tVariety\tVisibility\tVisibility
Data Warehouse and Business Intelligence\tWhat is NOT a goal of a data warehouse program?\tSupport Business Intelligence\tEnable effective decision-making\tReplace all source systems\tInnovate based on data insights\tReplace all source systems
Data Warehouse and Business Intelligence\tHow should metadata be handled in a data warehouse project?\tBuild it after deployment\tOnly document technical details\tBuild it with the warehouse and manage it ongoing\tLeave it to end users\tBuild it with the warehouse and manage it ongoing
Data Warehouse and Business Intelligence\tWhich of these is NOT an activity for data warehouse maintenance?\tMonitor load processes\tArchive all historical data\tTune BI performance\tManage release cycles\tArchive all historical data
Data Warehouse and Business Intelligence\tService Level Agreement (SLA) for Data Warehouse\tOnly specifies response times\tIncludes data retention and availability requirements\tIs optional for compliance\tNever changes after initial setup\tIncludes data retention and availability requirements
Data Warehouse and Business Intelligence\tHow does an Operational Data Store (ODS) differ from a Data Warehouse?\tODS contains historical data\tODS has higher latency\tODS contains current/near-term volatile data\tODS is never used for reporting\tODS contains current/near-term volatile data
Data Warehouse and Business Intelligence\tWhen selecting a BI tool, which factor is most important?\tLowest cost\tMatching tool capabilities to user needs\tVendor popularity\tTechnical complexity\tMatching tool capabilities to user needs
Data Warehouse and Business Intelligence\tWhich of these statements is NOT true of designing a dimensional model?\tUses fact and dimension tables\tOptimizes for query performance\tRequires full normalization\tEnables drill-down analysis\tRequires full normalization
Data Warehouse and Business Intelligence\tWhich of these is TRUE about defining high-quality metadata?\tOnly technical staff should create it\tIt should be built incrementally with the warehouse\tIt can be added after all development is complete\tBusiness users don't need it\tIt should be built incrementally with the warehouse
Data Warehouse and Business Intelligence\tWhat is one of the major differences between OLAP and OLTP systems?\tOLAP focuses on transaction processing\tOLTP supports multi-dimensional analysis\tOLAP supports analytical queries\tThey are identical in purpose\tOLAP supports analytical queries
Data Warehouse and Business Intelligence\tWhat is the role of the staging area in ETL processes?\tStore final transformed data\tProvide real-time analytics\tPerform data transformation and integration\tReplace source systems\tPerform data transformation and integration
Data Warehouse and Business Intelligence\tWhat is NOT a goal of data profiling?\tIdentify data quality issues\tUnderstand data relationships\tReplace source systems\tAssess data integration complexity\tReplace source systems
Data Warehouse and Business Intelligence\tWhat are conformed dimensions?\tDimensions specific to one data mart\tShared dimensions across multiple data marts\tUnstructured data elements\tOperational system tables\tShared dimensions across multiple data marts
Data Warehouse and Business Intelligence\tWhat is the key difference between batch and real-time data processing in warehouses?\tBatch is faster\tReal-time processes all data at once\tBatch processes data in scheduled intervals\tReal-time doesn't need transformation\tBatch processes data in scheduled intervals
Data Warehouse and Business Intelligence\tA Data Steward in a data warehouse project\tOnly handles technical issues\tDefines business rules for data transformation\tReplaces the need for data governance\tManages hardware infrastructure\tDefines business rules for data transformation
Data Warehouse and Business Intelligence\tOne of the main approaches to managing sensitive data in a warehouse is\tData masking or obfuscation\tStoring all data unencrypted\tSharing freely within organization\tDeleting historical records\tData masking or obfuscation
Data Warehouse and Business Intelligence\tAn area around the edge of an organization for experimental data analysis is known as?\tProduction warehouse\tSandbox environment\tOperational system\tStaging area\tSandbox environment
Data Warehouse and Business Intelligence\tWhich of these principles are NOT part of effective data warehouse governance?\tBusiness-driven processes\tRisk mitigation\tMaximizing data collection\tAlignment with compliance\tMaximizing data collection
Data Warehouse and Business Intelligence\tThe 3 Vs - Which V refers to the variety of data formats in big data?\tVolume\tVelocity\tVariety\tVeracity\tVariety
Data Warehouse and Business Intelligence\tWhen creating an ethical data strategy, which concern is NOT valid?\tData sensitivity\tUser consent\tMaximal data hoarding\tAccess controls\tMaximal data hoarding
Data Warehouse and Business Intelligence\tAccording to the text, why is metadata critical for data warehouse success?\tTo increase storage costs\tTo explain data meaning and lineage\tTo replace documentation\tTo reduce query performance\tTo explain data meaning and lineage
Data Warehouse and Business Intelligence\tWhich of these is a core idea of dimensional modeling per Kimball?\tNormalized entity relationships\tStar schemas with facts and dimensions\tOperational system integration\tHistorical data elimination\tStar schemas with facts and dimensions
Data Warehouse and Business Intelligence\tData warehouse archiving challenges include\tUsers viewing the warehouse as active archive\tEasy implementation\tNo user resistance\tAutomatic data deletion\tUsers viewing the warehouse as active archive
Data Warehouse and Business Intelligence\tHow do service level agreements help data warehouse operations?\tBy eliminating all performance issues\tBy setting clear expectations for response and availability\tBy removing the need for monitoring\tBy standardizing all tools\tBy setting clear expectations for response and availability
Data Warehouse and Business Intelligence\tThe highest level of data warehouse architecture planning is the:\tIndividual data mart\tDepartmental report\tEnterprise data model\tOperational system\tEnterprise data model
Data Warehouse and Business Intelligence\tWhich of these is NOT a principle for successful data warehouse programs?\tStrong business sponsorship\tClear business goals\tIgnoring user feedback\tCommitted business resources\tIgnoring user feedback
Data Warehouse and Business Intelligence\tA relationship label that states 'one-to-many' in data warehousing typically refers to\tFact-to-fact relationships\tDimension-to-dimension relationships\tDimension-to-fact relationships\tOperational system joins\tDimension-to-fact relationships
Data Warehouse and Business Intelligence\tThe process of maintaining historical versions of dimension data is known as:\tData archiving\tSlowly changing dimensions\tReal-time processing\tData masking\tSlowly changing dimensions
Data Warehouse and Business Intelligence\tIn data warehousing, what is the primary purpose of a surrogate key?\tReplace business keys for performance\tStore business definitions\tEnable real-time feeds\tReplace foreign keys\tReplace business keys for performance
"""

# ------------------------------------------------------------------
# 2.  Parse the raw string into a list of dicts
# ------------------------------------------------------------------
QUESTIONS = []
for line in RAW.strip().split('\n'):
    parts = line.split('\t')
    QUESTIONS.append({
        "category": parts[0],
        "question": parts[1],
        "options": parts[2:6],
        "correct": parts[6]
    })

# ------------------------------------------------------------------
# 3.  Streamlit session-state helpers
# ------------------------------------------------------------------
def init_state():
    if "idx" not in st.session_state:
        st.session_state.idx = 0
        st.session_state.score = 0
        st.session_state.answered = False
        st.session_state.choice = None

# ------------------------------------------------------------------
# 4.  Quiz logic
# ------------------------------------------------------------------
def show_question():
    q = QUESTIONS[st.session_state.idx]
    st.markdown(f"**{q['category']}**")
    st.write(f"Question {st.session_state.idx + 1} / {len(QUESTIONS)}")
    st.write(q["question"])

    # Radio buttons for answer choices
    choice = st.radio(
        "Select an answer:",
        q["options"],
        key=f"q{st.session_state.idx}",
        index=None if not st.session_state.answered else q["options"].index(st.session_state.choice)
    )

    col1, col2, col3 = st.columns([1, 1, 1])
    with col1:
        submitted = st.button("Submit", disabled=st.session_state.answered)
    with col2:
        next_btn = st.button("Next", disabled=not st.session_state.answered)

    if submitted and choice is not None:
        st.session_state.answered = True
        st.session_state.choice = choice
        if choice == q["correct"]:
            st.session_state.score += 1
            st.success("✅ Correct!")
        else:
            st.error(f"❌ Incorrect. Correct answer: **{q['correct']}**")
        st.rerun()

    if next_btn and st.session_state.answered:
        if st.session_state.idx < len(QUESTIONS) - 1:
            st.session_state.idx += 1
            st.session_state.answered = False
            st.session_state.choice = None
            st.rerun()
        else:
            # Quiz finished
            st.balloons()
            st.write("### 🎉 Quiz finished!")
            st.write(f"**Your score: {st.session_state.score} / {len(QUESTIONS)}**")
            if st.button("Restart Quiz"):
                for key in ["idx", "score", "answered", "choice"]:
                    if key in st.session_state:
                        del st.session_state[key]
                st.rerun()

# ------------------------------------------------------------------
# 5.  Main app
# ------------------------------------------------------------------
st.set_page_config(page_title="Data-Management Quiz (269 Q)", page_icon="📊")
st.title("📊 Complete Data-Management Knowledge Quiz")
init_state()
show_question()
